{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Spark's Structured APIs"
      ],
      "metadata": {
        "id": "QeTlr2ta0F0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create or open an existing Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark_Session_2\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "S_Ps5UIda-rl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use DataFrame API"
      ],
      "metadata": {
        "id": "4CEqOHM00TsQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GZ-5c-Ltz9yG"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import avg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = spark.createDataFrame([(\"Tom\", 10),(\"Jerry\", 20), (\"Lays\", 30)], schema = 'name string, age int')\n",
        "\n",
        "data_df.groupBy('name').avg().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS7MJ08F0VPc",
        "outputId": "93f7056b-539c-468f-f392-a5ff6d571d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+\n",
            "| name|avg(age)|\n",
            "+-----+--------+\n",
            "|  Tom|    10.0|\n",
            "| Lays|    30.0|\n",
            "|Jerry|    20.0|\n",
            "+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use RDD APIs\n",
        "\n",
        "Transformations used in the following WordCount eg: flatMap and map\n",
        "\n",
        "I learned to be careful with the spelling. M is upper case in 'flatMap'.\n"
      ],
      "metadata": {
        "id": "3mpczrxl9ufm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WordCount eg."
      ],
      "metadata": {
        "id": "tCe8jgn66w1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc"
      ],
      "metadata": {
        "id": "gJn0Yih3h-H7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1. Import a file\n",
        "# fileRDD = spark.sparkContext.textFile(\"file://home/file_folder/inputText\")\n",
        "\n",
        "# Method 2. Use a string to do word count\n",
        "fileRDD = spark.sparkContext.parallelize([\"hello word count hello\"])\n",
        "\n",
        "wordsDF = fileRDD.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, )).toDF(\"word string\")\n",
        "\n",
        "# or\n",
        "# from pyspark.sql.types import StringType\n",
        "# wordsDF = spark.createDataFrame(fileRDD.flatMap(lambda x: x.split(\" \")), stringType()).withColumnRenamed(\"value\", \"word\")\n",
        "\n",
        "countDF = wordsDF.groupBy(\"word\").count()\n",
        "\n",
        "# show output without sorting results in descending order\n",
        "countDF.show()\n",
        "\n",
        "# show output that is sorting results in descending order\n",
        "countDF.sort(desc(\"count\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zGeBVPA63GQ",
        "outputId": "92292e4e-6ad0-48e2-da77-e66890d04c15"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "| word|count|\n",
            "+-----+-----+\n",
            "|hello|    2|\n",
            "|count|    1|\n",
            "| word|    1|\n",
            "+-----+-----+\n",
            "\n",
            "+-----+-----+\n",
            "| word|count|\n",
            "+-----+-----+\n",
            "|hello|    2|\n",
            "|count|    1|\n",
            "| word|    1|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WordCount (RDD version) Eg."
      ],
      "metadata": {
        "id": "-SPxzumxxHgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf"
      ],
      "metadata": {
        "id": "q8VljcHfxHCo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart kernel if run into an error message of running multiple SparkContexts at once\n",
        "try:\n",
        "    sc.stop()\n",
        "except NameError:\n",
        "    pass"
      ],
      "metadata": {
        "id": "mkCDuModf7DF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf()                                        #create configuration for Spark job\n",
        "conf.setMaster(\"local\").setAppName(\"wordcount_rdd\")           #Run Spark locally, name your app\n",
        "sc = SparkContext(conf=conf)                              #creates a SparkContext (entry point for using Spark API)\n",
        "\n",
        "# or can do this directly\n",
        "# sc = SparkContext('local', \"wordcount\")\n",
        "\n",
        "# Method 1. Read from a file\n",
        "text = sc.textFile(\"text.txt\")                           #reads input file (a plan text file) into an RDD\n",
        "\n",
        "# Method 2. Create a line of text instead of reading from a file\n",
        "line = \"hi lets do word count , count again and again\"\n",
        "text = sc.parallelize([line])                            # convert list of strings into an RDD\n",
        "\n",
        "# split lines, make pairs, count words\n",
        "count = text.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Can either show results or save it to .csv\n",
        "\n",
        "# count.saveAsTextFile(\"results_rdd\")                    # output results\n",
        "\n",
        "for word, count in count.collect():\n",
        "    print(word, count)\n",
        "\n",
        "sc.stop()                                                # close Spark session"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S-nMcYZxMrj",
        "outputId": "706416cf-9dfd-4ddd-8926-02ba65efe91d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi 1\n",
            "lets 1\n",
            "do 1\n",
            "word 1\n",
            "count 2\n",
            ", 1\n",
            "again 2\n",
            "and 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WordCount (DataFrame API version) Eg\n",
        "\n",
        "-This version is more modern and optimized"
      ],
      "metadata": {
        "id": "87JQXjeo4m5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "VesA7BHexfFA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"WordCount\").master(\"local\").getOrCreate()      # Start Spark locally with an app name\n",
        "\n",
        "# Method 1. Read from a file\n",
        "#fileDF = spark.read.text(\"file_of_text.txt\")                                        # Read text files into a DataFrame\n",
        "\n",
        "# Method 2. Use a string to do word count\n",
        "data = [(\"hi lets do word count hi\",)]\n",
        "fileDF = spark.createDataFrame(data, [\"value\"])                                      # Column name set as 'value' to match rest of code\n",
        "\n",
        "wordsDF = fileDF.select(explode(split(fileDF.value, \" \")).alias(\"word\"))             # Split lines into words & flatten them\n",
        "countsDF = wordsDF.groupBy(\"word\").agg(count(\"*\").alias(\"count\"))                    #count how many times each word occurs\n",
        "\n",
        "# Can either show results or save it to .csv\n",
        "# countsDF.write.format(\"csv\").save(\"results\")                                       # Save results as CSV files\n",
        "\n",
        "countsDF.show()\n",
        "\n",
        "spark.stop()                                                                         # Stop Spark session"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXXZrqKP43IL",
        "outputId": "ef4d7f2d-4151-47fd-c726-75888c5caaf1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "| word|count|\n",
            "+-----+-----+\n",
            "|count|    1|\n",
            "| lets|    1|\n",
            "| word|    1|\n",
            "|   do|    1|\n",
            "|   hi|    2|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}