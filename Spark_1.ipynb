{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSaplCN6awqS",
        "outputId": "3bce7370-a071-4eff-b228-a313cb223b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create or open an existing Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark_Session_1\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "S_Ps5UIda-rl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataframes with schema:\n",
        "\n",
        "It is ideal to specify the name and data type for each column, otherwise Spark would need to infer from the data what is the data type for each column, then to assign these data types to columns. This makes Spark less efficient.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfOvW92rb6Sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Tom\", 99), (\"Jerry\", 99)]\n",
        "\n",
        "dataDF = spark.createDataFrame(data, schema = \"name string, age int\")\n",
        "\n",
        "dataDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tInyRMijcTDI",
        "outputId": "941f4f2a-4aa1-43bf-a94b-7af2030681f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|  Tom| 99|\n",
            "|Jerry| 99|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#StructType & StructField\n",
        "\n",
        "The StructType and StructField classes can be used to specify to the DataFrame about what is the schema.\n",
        "\n",
        "These classes are also helpful with creating columns that are more complicaited.\n"
      ],
      "metadata": {
        "id": "ByxIZpTkefxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n"
      ],
      "metadata": {
        "id": "kpRVZNZ-fRgR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([StructField(\"title\", StringType(), False),\n",
        "StructField(\"author\", StringType(), False),\n",
        "StructField(\"pages\", IntegerType(), False)])\n",
        "#schema = “title string, author string, pages int”\n",
        "\n",
        "data = [(\"first book\", \"1st author\", 500), (\"second book\", \"2nd author\", 700)]\n",
        "\n",
        "dataDF = spark.createDataFrame(data, schema)\n",
        "\n",
        "dataDF.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47WmeKWsfUFi",
        "outputId": "8a165205-48a3-4693-9d55-dd57b6a2e4ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----+\n",
            "|      title|    author|pages|\n",
            "+-----------+----------+-----+\n",
            "| first book|1st author|  500|\n",
            "|second book|2nd author|  700|\n",
            "+-----------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See each column's name and data type"
      ],
      "metadata": {
        "id": "ctX7vjbUfg8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataDF.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApG4_A0yfjxs",
        "outputId": "2ac70f4a-1aac-4a23-cd10-65bce1a2686e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- title: string (nullable = false)\n",
            " |-- author: string (nullable = false)\n",
            " |-- pages: integer (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rows"
      ],
      "metadata": {
        "id": "ZRH0Zt62br49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row"
      ],
      "metadata": {
        "id": "5ZdKksvvfZsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Given a list of rows containing names and ages\n",
        "\n",
        "row_data = [Row(name = \"Ben\", age= 99), Row(name = \"Jerry\", age = 99)]\n",
        "\n",
        "# Create DataFrame from a list of Rows\n",
        "\n",
        "dataDF = spark.createDataFrame(row_data)\n",
        "\n",
        "dataDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDwG2polbTb7",
        "outputId": "4b785543-6219-4a41-8d11-317b0627baa4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|  Ben| 99|\n",
            "|Jerry| 99|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}